{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8bde8d8",
   "metadata": {},
   "source": [
    "### 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982be060",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_corpus_bert\n",
    "\n",
    "TRAIN_PATH = \"./data/weibo/train.txt\"\n",
    "TEST_PATH = \"./data/weibo/test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad353dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分别加载训练集和测试集\n",
    "train_data = load_corpus_bert(TRAIN_PATH)\n",
    "test_data = load_corpus_bert(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd19293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.DataFrame(train_data, columns=[\"text\", \"label\"])\n",
    "df_test = pd.DataFrame(test_data, columns=[\"text\", \"label\"])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514c3b70",
   "metadata": {},
   "source": [
    "### 加载bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2723a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"    # 在我的电脑上不加这一句, bert模型会报错\n",
    "MODEL_PATH = \"./model/chinese_wwm_pytorch\"     # 下载地址见 https://github.com/ymcui/Chinese-BERT-wwm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f196c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)   # 分词器\n",
    "bert = BertModel.from_pretrained(MODEL_PATH)            # 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80410f86",
   "metadata": {},
   "source": [
    "### 神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d77f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "bert = bert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32438735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数\n",
    "learning_rate = 1e-3\n",
    "input_size = 768\n",
    "num_epoches = 10\n",
    "batch_size = 100\n",
    "decay_rate = 0.9\n",
    "hidden_size = 64\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85110024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.data = df[\"text\"].tolist()\n",
    "        self.label = df[\"label\"].tolist()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        label = self.label[index]\n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "# 训练集\n",
    "train_data = MyDataset(df_train)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 测试集\n",
    "test_data = MyDataset(df_test)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731b8abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络结构\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, 1)  # 双向, 输出维度要*2\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, (h_n, h_c) = self.lstm(x)\n",
    "        # print(len(lstm_out))\n",
    "        # print(h_n[-1].size())\n",
    "        # print(h_n[-2].size())\n",
    "        lstm_out_cat = torch.cat([h_n[-2], h_n[-1]], 1)  # 双向, 所以要将最后两维拼接, 得到的就是最后一个time step的输出\n",
    "        out = self.fc(lstm_out_cat)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "lstm = LSTM(input_size, hidden_size, num_layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e590ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# 测试集效果检验\n",
    "def test():\n",
    "    y_pred, y_true = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for words, labels in test_loader:\n",
    "            tokens = tokenizer(words, padding=True)\n",
    "            input_ids = torch.tensor(tokens[\"input_ids\"]).to(device)\n",
    "            attention_mask = torch.tensor(tokens[\"attention_mask\"]).to(device)\n",
    "            last_hidden_states = bert(input_ids, attention_mask=attention_mask)\n",
    "            bert_output = last_hidden_states[0]\n",
    "            outputs = lstm(bert_output)          # 前向传播\n",
    "            outputs = outputs.view(-1)          # 将输出展平\n",
    "            y_pred.append(outputs)\n",
    "            y_true.append(labels)\n",
    "\n",
    "    y_prob = torch.cat(y_pred)\n",
    "    y_true = torch.cat(y_true)\n",
    "    y_pred = y_prob.clone()\n",
    "    y_pred[y_pred > 0.5] = 1\n",
    "    y_pred[y_pred <= 0.5] = 0\n",
    "\n",
    "    y_true = y_true.cpu()\n",
    "    y_pred = y_pred.cpu()\n",
    "    y_prob = y_prob.cpu()\n",
    "    print(metrics.classification_report(y_true, y_pred))\n",
    "    print(\"准确率:\", metrics.accuracy_score(y_true, y_pred))\n",
    "    print(\"AUC:\", metrics.roc_auc_score(y_true, y_prob) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1157a290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数和优化器\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=decay_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357bce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 迭代训练\n",
    "for epoch in range(num_epoches):\n",
    "    total_loss = 0\n",
    "    for i, (words, labels) in enumerate(train_loader):\n",
    "        tokens = tokenizer(words, padding=True)\n",
    "        input_ids = torch.tensor(tokens[\"input_ids\"]).to(device)\n",
    "        attention_mask = torch.tensor(tokens[\"attention_mask\"]).to(device)\n",
    "        labels = labels.float().to(device)\n",
    "        with torch.no_grad():\n",
    "            last_hidden_states = bert(input_ids, attention_mask=attention_mask)\n",
    "            bert_output = last_hidden_states[0]\n",
    "        optimizer.zero_grad()               # 梯度清零\n",
    "        # print(bert_output.size()) 100*786\n",
    "        outputs = lstm(bert_output)          # 前向传播\n",
    "        logits = outputs.view(-1)           # 将输出展平\n",
    "        loss = criterion(logits, labels)    # loss计算\n",
    "        total_loss += loss\n",
    "        loss.backward()                     # 反向传播，计算梯度\n",
    "        optimizer.step()                    # 梯度更新\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(\"epoch:{}, step:{}, loss:{}\".format(epoch+1, i+1, total_loss/10))\n",
    "            total_loss = 0\n",
    "    \n",
    "    # learning_rate decay\n",
    "    scheduler.step()\n",
    "    \n",
    "    # test\n",
    "    test()\n",
    "    \n",
    "    # save model\n",
    "    model_path = \"./model/bert_lstm_dnn_{}.model\".format(epoch+1)\n",
    "    torch.save(lstm, model_path)\n",
    "    print(\"saved model: \", model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9fcf92",
   "metadata": {},
   "source": [
    "# 手动输入句子，判断情感倾向（1正/0负）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af165db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load(\"./model/bert_lstm_dnn_4.model\")    # 训练过程中的巅峰时刻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6ebaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [\"华丽繁荣的城市、充满回忆的小镇、郁郁葱葱的山谷...\", \"突然就觉得人间不值得\"]\n",
    "tokens = tokenizer(s, padding=True)\n",
    "input_ids = torch.tensor(tokens[\"input_ids\"]).to(device)\n",
    "attention_mask = torch.tensor(tokens[\"attention_mask\"]).to(device)\n",
    "\n",
    "last_hidden_states = bert(input_ids, attention_mask=attention_mask)\n",
    "bert_output = last_hidden_states[0]\n",
    "outputs = net(bert_output)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c5ab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [\"今天天气真好\", \"今天天气特别特别棒\"]\n",
    "tokens = tokenizer(s, padding=True)\n",
    "input_ids = torch.tensor(tokens[\"input_ids\"]).to(device)\n",
    "attention_mask = torch.tensor(tokens[\"attention_mask\"]).to(device)\n",
    "\n",
    "last_hidden_states = bert(input_ids, attention_mask=attention_mask)\n",
    "bert_output = last_hidden_states[0]\n",
    "outputs = net(bert_output)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d787a39e",
   "metadata": {},
   "source": [
    "# 单句部署"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3b1f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_PATH = \"./model/chinese_wwm_pytorch\"     # 下载地址见 https://github.com/ymcui/Chinese-BERT-wwm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19db8933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络结构\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, 1)  # 双向, 输出维度要*2\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, (h_n, h_c) = self.lstm(x)\n",
    "        # print(len(lstm_out))\n",
    "        # print(h_n[-1].size())\n",
    "        # print(h_n[-2].size())\n",
    "        lstm_out_cat = torch.cat([h_n[-2], h_n[-1]], 1)  # 双向, 所以要将最后两维拼接, 得到的就是最后一个time step的输出\n",
    "        out = self.fc(lstm_out_cat)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f67951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)   # 分词器\n",
    "bert = BertModel.from_pretrained(MODEL_PATH).to(device)            # 模型\n",
    "net = torch.load(\"./model/bert_lstm_dnn_7.model\")    # 训练过程中的巅峰时刻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1d0780",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [\"天天重复的生活真没意思\"]\n",
    "tokens = tokenizer(s, padding=True)\n",
    "input_ids = torch.tensor(tokens[\"input_ids\"]).to(device)\n",
    "attention_mask = torch.tensor(tokens[\"attention_mask\"]).to(device)\n",
    "\n",
    "last_hidden_states = bert(input_ids, attention_mask=attention_mask)\n",
    "bert_output = last_hidden_states[0]\n",
    "outputs = net(bert_output)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfb264f",
   "metadata": {},
   "source": [
    "# 批量部署"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05971874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from utils import load_corpus_bert\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_PATH = \"./model/chinese_wwm_pytorch\"     # 下载地址见 https://github.com/ymcui/Chinese-BERT-wwm\n",
    "TEST_PATH = \"./data/weibo/test.txt\"\n",
    "\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930bc9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载测试数据集\n",
    "test_data = load_corpus_bert(TEST_PATH)\n",
    "df_test = pd.DataFrame(test_data, columns=[\"text\", \"label\"])\n",
    "\n",
    "# 数据集\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.data = df[\"text\"].tolist()\n",
    "        self.label = df[\"label\"].tolist()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        label = self.label[index]\n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "# 测试集\n",
    "test_data = MyDataset(df_test)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb960d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络结构\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, 1)  # 双向, 输出维度要*2\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, (h_n, h_c) = self.lstm(x)\n",
    "        # print(len(lstm_out))\n",
    "        # print(h_n[-1].size())\n",
    "        # print(h_n[-2].size())\n",
    "        lstm_out_cat = torch.cat([h_n[-2], h_n[-1]], 1)  # 双向, 所以要将最后两维拼接, 得到的就是最后一个time step的输出\n",
    "        out = self.fc(lstm_out_cat)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc44873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# 测试集效果检验\n",
    "def test():\n",
    "    y_pred, y_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for words, labels in test_loader:\n",
    "            tokens = tokenizer(words, padding=True)\n",
    "            input_ids = torch.tensor(tokens[\"input_ids\"]).to(device)\n",
    "            attention_mask = torch.tensor(tokens[\"attention_mask\"]).to(device)\n",
    "            last_hidden_states = bert(input_ids, attention_mask=attention_mask)\n",
    "            bert_output = last_hidden_states[0]\n",
    "            outputs = net(bert_output)          # 前向传播\n",
    "            outputs = outputs.view(-1)          # 将输出展平\n",
    "            y_pred.append(outputs)\n",
    "            y_true.append(labels)\n",
    "\n",
    "    y_prob = torch.cat(y_pred)\n",
    "    y_true = torch.cat(y_true)\n",
    "    y_pred = y_prob.clone()\n",
    "    y_pred[y_pred > 0.5] = 1\n",
    "    y_pred[y_pred <= 0.5] = 0\n",
    "\n",
    "    y_true = y_true.cpu()\n",
    "    y_pred = y_pred.cpu()\n",
    "    y_prob = y_prob.cpu()\n",
    "    print(metrics.classification_report(y_true, y_pred))\n",
    "    print(\"准确率:\", metrics.accuracy_score(y_true, y_pred))\n",
    "    print(\"AUC:\", metrics.roc_auc_score(y_true, y_prob) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced50f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)   # 分词器\n",
    "bert = BertModel.from_pretrained(MODEL_PATH).to(device)            # 模型\n",
    "net = torch.load(\"./model/bert_lstm_dnn_7.model\")    # 训练过程中的巅峰时刻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdac872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18eb666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1b8ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
